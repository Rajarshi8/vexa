# VEXA AI Agent

ğŸ¤– **VEXA** is a powerful, privacy-focused, and fully offline-capable AI Agent designed to perform intelligent tasks such as answering queries, searching the web, and using custom tools. Built using LangChain, Ollama, and DuckDuckGo, it utilizes open-source LLMs (like Mistral, LLaMA 2) to replicate ChatGPT-like functionality â€“ without internet dependency, without API keys, and 100% free.

## ğŸ’¡ Key Features

âœ… **Runs Locally**: Uses open-source LLMs via Ollama â€” no need for paid APIs  
ğŸ” **Web Search Tool**: Integrated with DuckDuckGo for real-time answers and current events  
ğŸ¤– **LangChain Agent**: Powered by the ReAct Agent with tool chaining  
ğŸ–¥ï¸ **Command-line & Web UI**: Interact via terminal or simple Gradio interface  
ğŸ”§ **Modular & Scalable**: Easily plug in your own tools (PDF reader, calculator, etc.)  
ğŸ”’ **Private & Offline-Ready**: Great for local development and secure environments  

## ğŸ§± Tech Stack

| Layer | Tool |
|-------|------|
| ğŸ’¬ **LLM** | Ollama â€“ Mistral / LLaMA 2 |
| ğŸ¤– **Agent Framework** | LangChain |
| ğŸŒ **Search Tool** | DuckDuckGo Search API |
| âš™ï¸ **Programming** | Python |
| ğŸŒ **Optional UI** | Gradio |

## ğŸ§ª How It Works

1. **Ollama** loads a local LLM (e.g., `mistral`) to serve as the brain of the agent
2. **LangChain** initializes an agent with custom tools like DuckDuckGo search
3. The user inputs a question via terminal or web UI
4. The agent decides whether to:
   - Answer directly using the LLM, or
   - Use a tool like search, then respond
5. Output is returned intelligently, step-by-step

## ğŸ“ Project Structure

```
vexa/
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ __init__.py        # Package initialization
â”‚   â”œâ”€â”€ config.py          # Model and configuration settings
â”‚   â”œâ”€â”€ core.py            # Agent logic using LangChain
â”‚   â””â”€â”€ tools.py           # Tools like DuckDuckGo, calculator, etc.
â”œâ”€â”€ app/
â”‚   â””â”€â”€ run_agent.py       # CLI runner
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ web_ui.py          # Gradio web interface (optional)
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md             # This file
â””â”€â”€ ai_agent.py           # Original simple version (legacy)
```

## ğŸš€ Quick Start

### Prerequisites

1. **Install Ollama**:
   ```bash
   # Visit https://ollama.ai/ or install via:
   # Windows: Download from website
   # macOS: brew install ollama
   # Linux: curl -fsSL https://ollama.ai/install.sh | sh
   ```

2. **Pull a model**:
   ```bash
   ollama pull mistral
   # or
   ollama pull llama2
   ```

3. **Start Ollama server**:
   ```bash
   ollama serve
   ```

### Installation

1. **Clone/Download the project**:
   ```bash
   git clone <repository-url>
   cd vexa
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

### Usage Options

#### Option 1: Command Line Interface

```bash
# Basic usage with default model (mistral)
python app/run_agent.py

# Use specific model
python app/run_agent.py --model llama2

# Single query mode
python app/run_agent.py --query "What's the weather like today?"

# Show available tools
python app/run_agent.py --tools-info

# Verbose mode for debugging
python app/run_agent.py --verbose
```

#### Option 2: Web Interface (Gradio)

```bash
# Launch web UI with default settings
python ui/web_ui.py

# Custom port and model
python ui/web_ui.py --model llama2 --port 8080

# Share publicly (creates public link)
python ui/web_ui.py --share
```

Then open your browser to `http://localhost:7860` (or specified port).

#### Option 3: Python Integration

```python
from agent import create_vexa_agent

# Create agent instance
agent = create_vexa_agent(model_name="mistral")

# Ask questions
response = agent.query("What's the current date?")
print(response["response"])

# Search the web
response = agent.query("Latest AI news 2024")
print(response["response"])
```

## ğŸ› ï¸ Available Tools

- **ğŸ” Web Search**: Search the web using DuckDuckGo
- **ğŸ§® Calculator**: Perform mathematical calculations
- **ğŸ“… DateTime**: Get current date and time information  
- **ğŸ“ File Operations**: Basic file system operations
- **ğŸŒ¤ï¸ Weather**: Weather information (placeholder for demo)

## ğŸ’¬ Example Interactions

```
ğŸ¤” Ask me anything: What's 15 * 23?
ğŸ¤– VEXA: I'll calculate that for you.
Result: 345

ğŸ¤” Ask me anything: What's the latest news about AI?
ğŸ¤– VEXA: Let me search for the latest AI news...
[Searches web and provides current information]

ğŸ¤” Ask me anything: What time is it?
ğŸ¤– VEXA: Current date and time: 2024-08-08 14:30:25
```

## ğŸ”§ Customization

### Adding Custom Tools

Create your custom tool:

```python
from langchain_core.tools import Tool

def my_custom_function(input_text: str) -> str:
    return f"Processed: {input_text}"

custom_tool = Tool(
    name="my_tool",
    func=my_custom_function,
    description="Description of what this tool does"
)

# Add to agent
agent.add_tool(custom_tool)
```

### Configuration

Edit `agent/config.py` to modify:
- Default models
- Agent settings
- UI configuration
- Tool parameters

## ğŸš¨ Troubleshooting

### Common Issues

1. **"Failed to initialize agent"**:
   - Ensure Ollama is running: `ollama serve`
   - Check if model is installed: `ollama list`
   - Install required model: `ollama pull mistral`

2. **"Model not found"**:
   - Use `python app/run_agent.py --models` to see available models
   - Pull the model: `ollama pull <model-name>`

3. **Web search not working**:
   - Check internet connection
   - DuckDuckGo might have rate limits

4. **Web UI not loading**:
   - Install gradio: `pip install gradio>=4.0.0`
   - Check if port is available
   - Try a different port: `--port 8080`

### Performance Tips

- **For faster responses**: Use smaller models like `mistral` or `orca-mini`
- **For better quality**: Use larger models like `llama2` or `llama3`
- **For coding tasks**: Use `codellama` model

## ğŸš€ Use Cases

- ğŸ‘¨â€ğŸ’» **Personal productivity assistant**
- ğŸ§‘â€ğŸ« **Study buddy for offline environments**  
- ğŸ”’ **Secure AI assistant for private organizations**
- ğŸ§ª **Testing LangChain agent logic without paid APIs**
- ğŸŒ **Local chatbot with web search capabilities**
- ğŸ“Š **Data analysis and calculations**

## ğŸ”’ Privacy & Security

- **100% Local Processing**: All queries processed on your machine
- **No Data Collection**: No telemetry or data sent to external servers
- **Offline Capable**: Works without internet (except for web search)
- **Open Source**: Full transparency of code and models

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is open source. See LICENSE file for details.

## ğŸ™ Acknowledgments

- **Ollama** for local LLM serving
- **LangChain** for agent framework
- **DuckDuckGo** for search capabilities
- **Gradio** for web interface
- **Open Source LLM Community** for the models

## ğŸ“ Support

If you encounter issues:

1. Check the troubleshooting section above
2. Ensure all dependencies are installed
3. Verify Ollama is running with the correct model
4. Check GitHub issues for similar problems

---

**Made with â¤ï¸ for the open-source AI community**
